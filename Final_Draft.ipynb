{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNrVNI3OuFucdVmO1huHRsx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joeyeuron/Case-Study-CE880/blob/main/Final_Draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "MZj9H3SNpPar"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A18gBM3YXZ-y"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "from pandas.plotting import scatter_matrix\n",
        "import matplotlib.pyplot as plt # data visualization\n",
        "import seaborn as sns # statistical data visualization\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_curve, roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Exploration"
      ],
      "metadata": {
        "id": "FH_elpghlEfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "nM5vBjQekpdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset using pandas"
      ],
      "metadata": {
        "id": "NLr2U89NoIM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "excel_file = pd.ExcelFile('loan_data.xlsx')\n",
        "df = excel_file.parse(excel_file.sheet_names[0])"
      ],
      "metadata": {
        "id": "3oP9LtHllKbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Perform Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "JrSolUa9oalW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "id": "dDnqEDieonMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic information about the dataset\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "Ia-h9GQKHt2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Statistical summary of the numerical columns\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "Vo67nnM1H1ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Distribution of the target variable 'Status'\n",
        "print(df['Status'].value_counts())"
      ],
      "metadata": {
        "id": "3Rr5QATKH8ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preprocessing"
      ],
      "metadata": {
        "id": "xHXB297zo2nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Missing Values\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "iWOh6k-OIEuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Handled missing values"
      ],
      "metadata": {
        "id": "OkhF9ikQmuwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing missing values with mode for respective columns\n",
        "df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n",
        "df['Married'].fillna(df['Married'].mode()[0], inplace=True)\n",
        "df['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)\n",
        "df['Self_Employed'].fillna(df['Self_Employed'].mode()[0], inplace=True)\n",
        "df['Term'].fillna(df['Term'].mean(), inplace=True)\n",
        "df['Credit_History'].fillna(df['Credit_History'].mode()[0], inplace=True)\n"
      ],
      "metadata": {
        "id": "Sg-81yiVILrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Encoded categorical variables"
      ],
      "metadata": {
        "id": "XbqOH2wpm67t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Dependents' column to strings\n",
        "df['Dependents'] = df['Dependents'].astype(str)\n",
        "\n",
        "# Using OneHotEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Convert 'Dependents' column to strings\n",
        "df['Dependents'] = df['Dependents'].astype(str)\n",
        "\n",
        "# Using OneHotEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#Categorical columns for one-hot encoding (excluding 'Status')\n",
        "categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Area']\n",
        "\n",
        "onehot_encoder = OneHotEncoder()\n",
        "encoded_features = onehot_encoder.fit_transform(df[categorical_columns])\n",
        "feature_names = onehot_encoder.get_feature_names_out(input_features=categorical_columns)\n",
        "df_encoded = pd.concat([df.drop(categorical_columns, axis=1), pd.DataFrame(encoded_features.toarray(), columns=feature_names)], axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "IGRGblVHLJBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Visualization"
      ],
      "metadata": {
        "id": "YF_2kn3TpE7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Histogram for Applicant_Income\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(df_encoded['Applicant_Income'], bins=20, color='blue', edgecolor='black')\n",
        "plt.xlabel('Applicant Income')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Histogram of Applicant Income')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M0y4iQlMlVe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Bar Chart for Credit_History\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='Credit_History', data=df_encoded, palette='pastel')\n",
        "plt.xlabel('Credit History')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Count of Credit History')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "070i0vAilerA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Correlation Heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "corr_matrix = df_encoded.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KoeNijQWljcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Scatter Plot for Applicant_Income vs. Loan_Amount\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df_encoded['Applicant_Income'], df_encoded['Loan_Amount'], c=df_encoded['Status'].map({'Y': 'blue', 'N': 'red'}), alpha=0.6)\n",
        "plt.xlabel('Applicant Income')\n",
        "plt.ylabel('Loan Amount')\n",
        "plt.title('Scatter Plot: Applicant Income vs. Loan Amount')\n",
        "plt.legend(['Approved', 'Not Approved'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dS2SggdFlslh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Box Plot for Loan_Amount by Status\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.boxplot(x='Status', y='Loan_Amount', data=df_encoded, palette='pastel')\n",
        "plt.xlabel('Status')\n",
        "plt.ylabel('Loan Amount')\n",
        "plt.title('Box Plot: Loan Amount by Status')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oKGT2V7PluZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Splitting"
      ],
      "metadata": {
        "id": "kledPU1glTfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split features (X) and the target variable (y)\n",
        "X = df_encoded.drop('Status', axis=1)\n",
        "y = df_encoded['Status']\n",
        "\n",
        "# Split data into training (80%), validation (10%), and test sets (10%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "pNwno4fQji08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Selection and Training"
      ],
      "metadata": {
        "id": "uawKq34NlXX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling feature Data for Model"
      ],
      "metadata": {
        "id": "SXSF13e4rRM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fitting the scaler on the training data and transforming both training and validation data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Scaling the test data using the same scaler used for training data\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "MHdqM65snFv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Initialized classification models"
      ],
      "metadata": {
        "id": "AAj6y0YWneIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model = SVC(probability=True)\n",
        "rf_model = RandomForestClassifier()\n",
        "knn_model = KNeighborsClassifier()\n",
        "gb_model = GradientBoostingClassifier()\n",
        "nb_model = GaussianNB()"
      ],
      "metadata": {
        "id": "GHE-vw-2fUk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defined parameter grids for grid search to tune hyperparameters"
      ],
      "metadata": {
        "id": "p-w63eRPoMkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining parameter grids for grid search\n",
        "svm_param_grid = {\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': [0.1, 1, 10]\n",
        "}\n",
        "\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "knn_param_grid = {\n",
        "    'n_neighbors': [3, 5, 7],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'p': [1, 2]\n",
        "}\n",
        "\n",
        "gb_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.1, 0.01, 0.001],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "nb_param_grid = {\n",
        "    'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
        "}\n",
        "\n",
        "# List of models and their respective parameter grids\n",
        "models = [svm_model, rf_model, knn_model, gb_model, nb_model]\n",
        "param_grids = [svm_param_grid, rf_param_grid, knn_param_grid, gb_param_grid, nb_param_grid]\n",
        "model_names = ['SVM', 'Random Forest', 'KNN', 'Gradient Boosting', 'Naive Bayes']\n"
      ],
      "metadata": {
        "id": "VmmWJLqloI0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Selection of best model"
      ],
      "metadata": {
        "id": "ILHkHRuqF6EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_models = []  # To store the best models after grid search\n",
        "\n",
        "# Loop over each model and its corresponding parameter grid\n",
        "for model, param_grid, model_name in zip(models, param_grids, model_names):\n",
        "    # Initialize GridSearchCV with the current model, parameter grid, and 5-fold cross-validation\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1') # Selection base don F1 score\n",
        "\n",
        "    # Fit the GridSearchCV object to the training data\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Retrieivng best model and its hyperparameters from the grid search\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_models.append(best_model)\n",
        "    best_hyperparameters = grid_search.best_params_\n",
        "\n",
        "    # Print the best hyperparameters for the current model\n",
        "    print(f\"Best Hyperparameters for {model_name}:\")\n",
        "    print(best_hyperparameters)\n"
      ],
      "metadata": {
        "id": "_zRPcJ73CtGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Evaluation on Validation Set"
      ],
      "metadata": {
        "id": "9TWJD9CEpPcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluated each best model on the validation set"
      ],
      "metadata": {
        "id": "S_5KgxI0ptuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to store evaluation metrics\n",
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "\n",
        "# Iteration over each best model\n",
        "for best_model, model_name in zip(best_models, model_names):\n",
        "    # Predict on the validation set\n",
        "    y_pred_val = best_model.predict(X_val_scaled)\n",
        "\n",
        "    # Calculating and storing evaluation metrics\n",
        "    accuracy_scores.append(np.round(best_model.score(X_val_scaled, y_val), 3))\n",
        "    precision_scores.append(np.round(precision_score(y_val, y_pred_val, pos_label='Y'), 3))\n",
        "    recall_scores.append(np.round(recall_score(y_val, y_pred_val, pos_label='Y'), 3))\n",
        "    f1_scores.append(np.round(f1_score(y_val, y_pred_val, pos_label='Y'), 3))\n",
        "\n",
        "    # Print classification report\n",
        "    print(f\"Classification Report - {model_name} Model:\")\n",
        "    print(classification_report(y_val, y_pred_val, target_names=['N', 'Y']))\n",
        "\n",
        "    # Confusion matrix heatmap\n",
        "    cm = confusion_matrix(y_val, y_pred_val)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix - {model_name} Model')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "4X-orBYtDQK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# List of model names\n",
        "model_names = ['SVM', 'Random Forest', 'KNN', 'Gradient Boosting', 'Naive Bayes']\n",
        "\n",
        "# List of corresponding metrics\n",
        "accuracy_scores = [...]  # Replace with your actual accuracy scores\n",
        "precision_scores = [...]  # Replace with your actual precision scores\n",
        "recall_scores = [...]  # Replace with your actual recall scores\n",
        "f1_scores = [...]  # Replace with your actual F1 scores\n",
        "\n",
        "# Set positions for the bars\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.2\n",
        "\n",
        "# Create bar plots for each metric\n",
        "plt.bar(x - width, accuracy_scores, width, label='Accuracy')\n",
        "plt.bar(x, precision_scores, width, label='Precision')\n",
        "plt.bar(x + width, recall_scores, width, label='Recall')\n",
        "plt.bar(x + 2 * width, f1_scores, width, label='F1-Score')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Model Evaluation Metrics')\n",
        "plt.xticks(x, model_names)\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OhNjg0hAtaYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cross-validation"
      ],
      "metadata": {
        "id": "VZVjwfCQpxMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty lists to store mean scores for each model\n",
        "mean_accuracy_scores = []\n",
        "mean_precision_scores = []\n",
        "mean_recall_scores = []\n",
        "mean_f1_scores = []\n",
        "\n",
        "# Perform cross-validation for each model\n",
        "for model in models:\n",
        "    accuracy_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "    mean_accuracy_scores.append(np.mean(accuracy_scores))\n",
        "\n",
        "    try:\n",
        "        precision_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='precision')\n",
        "        mean_precision_scores.append(np.mean(precision_scores))\n",
        "    except:\n",
        "        mean_precision_scores.append(0)  # Replace with 0 or another suitable value\n",
        "\n",
        "    try:\n",
        "        recall_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='recall')\n",
        "        mean_recall_scores.append(np.mean(recall_scores))\n",
        "    except:\n",
        "        mean_recall_scores.append(0)  # Replace with 0 or another suitable value\n",
        "\n",
        "    try:\n",
        "        f1_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='f1')\n",
        "        mean_f1_scores.append(np.mean(f1_scores))\n",
        "    except:\n",
        "        mean_f1_scores.append(0)  # Replace with 0 or another suitable value\n",
        "\n",
        "\n",
        "# Display the mean cross-validation scores for each model\n",
        "for model_name, accuracy, precision, recall, f1 in zip(model_names, mean_accuracy_scores, mean_precision_scores, mean_recall_scores, mean_f1_scores):\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Mean Accuracy: {accuracy}\")\n",
        "    print(f\"Mean Precision: {precision}\")\n",
        "    print(f\"Mean Recall: {recall}\")\n",
        "    print(f\"Mean F1-Score: {f1}\")\n",
        "    print(\"=============================\")\n"
      ],
      "metadata": {
        "id": "6fY7cA7nDlYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Comparison using ROC AUC"
      ],
      "metadata": {
        "id": "rlU6OroLqLvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing empty lists to store mean ROC AUC scores for each model\n",
        "mean_roc_auc_scores = []\n",
        "\n",
        "# Performing cross-validation for each model\n",
        "for model in models:\n",
        "    roc_auc_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
        "    mean_roc_auc_scores.append(np.mean(roc_auc_scores))\n",
        "\n",
        "# Displaying the mean ROC AUC scores for each model\n",
        "for model_name, roc_auc in zip(model_names, mean_roc_auc_scores):\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Mean ROC AUC Score: {roc_auc}\")\n",
        "    print(\"=============================\")\n"
      ],
      "metadata": {
        "id": "0YqIGDRxIZIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean ROC AUC scores from cross-validation\n",
        "mean_roc_auc_scores = [0.7846481661130219, 0.7955742716876857, 0.67795847174452, 0.7553687616975825, 0.7564197313108153]\n",
        "model_names = ['SVM', 'Random Forest', 'KNN', 'Gradient Boosting', 'Naive Bayes']\n",
        "\n",
        "# Bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(model_names, mean_roc_auc_scores, color= ['skyblue', 'orange', 'green', 'red', 'purple'])\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Mean ROC AUC Score')\n",
        "plt.title('Mean ROC AUC Scores for Different Models')\n",
        "plt.ylim([0, 1])  # Set the y-axis range from 0 to 1\n",
        "plt.xticks(rotation=45)  # Rotated x-axis labels for better visibility\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wH0QoY-0InsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Best Model Evaluation on Test Set"
      ],
      "metadata": {
        "id": "vlrPujozqpI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Index of the Random Forest model in the list of model names\n",
        "rf_model_index = model_names.index('Random Forest')\n",
        "\n",
        "# Retrieve the best Random Forest model from the best_models list\n",
        "best_rf_model = best_models[rf_model_index]\n",
        "\n",
        "# Testing the best-performing Random Forest model on the test data\n",
        "y_pred_test_rf = best_rf_model.predict(X_test_scaled)\n",
        "\n",
        "# Classification report for the test set\n",
        "print(\"Classification Report - Best Random Forest Model (Test Set):\")\n",
        "print(classification_report(y_test, y_pred_test_rf, target_names=['N', 'Y']))\n",
        "\n",
        "# Confusion matrix heatmap for the test set\n",
        "cm_test_rf = confusion_matrix(y_test, y_pred_test_rf)\n",
        "sns.heatmap(cm_test_rf, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Best Random Forest Model (Test Set)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "AzC_dlGSD05r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Analyzing Feature Importance"
      ],
      "metadata": {
        "id": "1vppwkCzt2aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importances = best_rf_model.feature_importances_\n",
        "sorted_indices = np.argsort(feature_importances)[::-1]\n",
        "\n",
        "# Feature importance scores and corresponding feature names\n",
        "for idx in sorted_indices:\n",
        "    print(f\"{X.columns[idx]}: {feature_importances[idx]}\")\n"
      ],
      "metadata": {
        "id": "Vg09HYOUt_Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results\n",
        "\n",
        "1.   Precision: The precision for class 'N' (No) is 1.00, meaning that when the model predicts a loan as 'N', it is correct 100% of the time. However, the precision for class 'Y' (Yes) is 0.72, indicating that when the model predicts a loan as 'Y', it is correct 72% of the time.\n",
        "2.   Recall: The recall for class 'N' is 0.38, indicating that the model correctly identifies only 38% of the actual 'N' loans. However, the recall for class 'Y' is 1.00, meaning that the model correctly identifies all the actual 'Y' loans.\n",
        "3. F1-Score: The F1-score is a balanced metric that considers both precision and recall. The F1-score for class 'N' is 0.55, and for class 'Y' is 0.84.\n",
        "4. Support: The support is the number of samples in each class in the test set. There are 24 samples for class 'N' and 38 samples for class 'Y'.\n",
        "5. Accuracy: The overall accuracy of the model on the test set is 0.76, meaning that the model correctly predicts the loan status for 76% of the samples in the test set.\n",
        "6. Macro Avg: The macro-average takes the average of precision, recall, and F1-score for both classes, treating each class equally. The macro-average precision is 0.86, recall is 0.69, and F1-score is 0.69.\n",
        "7. Weighted Avg: The weighted average considers the support (number of samples) for each class and calculates a weighted average of precision, recall, and F1-score. The weighted average precision is 0.83, recall is 0.76, and F1-score is 0.72.\n",
        "\n",
        "The Random Forest model has good precision and recall for predicting the 'Y' class (loan approval), suggesting that it's able to accurately identify applicants who are likely to get approved for loans.\n",
        "The precision for class 'Y' is higher than class 'N', indicating that when the model predicts an applicant will get approved, it's more likely to be accurate.\n",
        "The recall for class 'Y' is high as well, meaning that the model is able to capture a significant proportion of actual loan approvals.\n",
        "The F1-score balances both precision and recall, providing a holistic view of the model's effectiveness."
      ],
      "metadata": {
        "id": "M6kZN_uWWacm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zStgwCgTt0w6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}